{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWgrF89a3D-4"
      },
      "source": [
        "# RAG Question-Answering System with Groq and Milvus Lite\n",
        "\n",
        "This notebook implements a RAG (Retrieval-Augmented Generation) system using Groq for LLM inference and Milvus Lite for vector storage. The system processes PDF documents, stores embeddings, and answers questions based on the document content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnED6uO23hYg"
      },
      "outputs": [],
      "source": [
        "!pip install PyPDF2 groq gradio nltk sentence-transformers pymilvus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTqiIT9c3D-6"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import PyPDF2\n",
        "import groq\n",
        "import gradio as gr\n",
        "from typing import List, Dict\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pymilvus import MilvusClient\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuMymQTa3D-7"
      },
      "source": [
        "## Document Processor\n",
        "\n",
        "This class handles PDF reading and text chunking operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GZJbV1Bw3D-7"
      },
      "outputs": [],
      "source": [
        "class DocumentProcessor:\n",
        "    def __init__(self):\n",
        "        nltk.download('punkt', quiet=True)\n",
        "\n",
        "    def read_pdf(self, file_path: str) -> str:\n",
        "        with open(file_path, 'rb') as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            text = ' '.join([page.extract_text() for page in reader.pages])\n",
        "        return text\n",
        "\n",
        "    def chunk_text(self, text: str, chunk_size: int = 1024, overlap_size: int = 100) -> List[str]:\n",
        "        \"\"\"\n",
        "        Split text into chunks with minimal overlap and clear boundaries.\n",
        "\n",
        "        Args:\n",
        "            text (str): Input text to be chunked\n",
        "            chunk_size (int): Target size for each chunk in characters\n",
        "            overlap_size (int): Number of characters to overlap between chunks\n",
        "\n",
        "        Returns:\n",
        "            List[str]: List of text chunks\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return []\n",
        "\n",
        "        # Clean and normalize text\n",
        "        text = ' '.join(text.split())  # Normalize whitespace\n",
        "        sentences = sent_tokenize(text)\n",
        "\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_length = 0\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence = sentence.strip()\n",
        "            sentence_length = len(sentence)\n",
        "\n",
        "            # If adding this sentence would exceed chunk size\n",
        "            if current_length + sentence_length > chunk_size and current_chunk:\n",
        "                # Store the current chunk\n",
        "                chunks.append(' '.join(current_chunk))\n",
        "\n",
        "                # Find sentences to keep for overlap\n",
        "                overlap_text = ' '.join(current_chunk)[-overlap_size:]\n",
        "                overlap_sentences = [s for s in current_chunk\n",
        "                                  if s in overlap_text][-2:]  # Keep up to 2 sentences\n",
        "\n",
        "                # Start new chunk with overlap sentences\n",
        "                current_chunk = overlap_sentences + [sentence]\n",
        "                current_length = sum(len(s) for s in current_chunk)\n",
        "            else:\n",
        "                current_chunk.append(sentence)\n",
        "                current_length += sentence_length\n",
        "\n",
        "        # Add the last chunk if it exists\n",
        "        if current_chunk:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "\n",
        "        # Post-process chunks to ensure quality\n",
        "        processed_chunks = []\n",
        "        for chunk in chunks:\n",
        "            # Remove any chunks that are too small\n",
        "            if len(chunk) >= chunk_size / 4:  # Minimum chunk size threshold\n",
        "                # Clean up chunk\n",
        "                chunk = chunk.strip()\n",
        "                if chunk:\n",
        "                    processed_chunks.append(chunk)\n",
        "\n",
        "        return processed_chunks\n",
        "\n",
        "    def get_chunk_stats(self, chunks: List[str]) -> Dict:\n",
        "        \"\"\"\n",
        "        Get statistics about the chunks for validation.\n",
        "\n",
        "        Args:\n",
        "            chunks (List[str]): List of text chunks\n",
        "\n",
        "        Returns:\n",
        "            Dict: Statistics about the chunks\n",
        "        \"\"\"\n",
        "        if not chunks:\n",
        "            return {\n",
        "                \"num_chunks\": 0,\n",
        "                \"avg_chunk_size\": 0,\n",
        "                \"min_chunk_size\": 0,\n",
        "                \"max_chunk_size\": 0\n",
        "            }\n",
        "\n",
        "        chunk_sizes = [len(chunk) for chunk in chunks]\n",
        "        return {\n",
        "            \"num_chunks\": len(chunks),\n",
        "            \"avg_chunk_size\": sum(chunk_sizes) / len(chunks),\n",
        "            \"min_chunk_size\": min(chunk_sizes),\n",
        "            \"max_chunk_size\": max(chunk_sizes)\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uq7eRrU13D-8"
      },
      "source": [
        "## RAG System\n",
        "\n",
        "This class implements the main RAG system functionality, including vector storage, embedding generation, and question answering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0ywf1nwO-8FQ"
      },
      "outputs": [],
      "source": [
        "from pymilvus import CollectionSchema, FieldSchema, DataType\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_pE-lBAP6Mx9"
      },
      "outputs": [],
      "source": [
        "class RAGSystem:\n",
        "    def __init__(self):\n",
        "        try:\n",
        "            # Existing initialization code remains the same\n",
        "            self.embedding_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\", trust_remote_code=True)\n",
        "            self.embedding_dim = 384\n",
        "            self.doc_processor = DocumentProcessor()\n",
        "            self.client = MilvusClient('milvus_demo.db') #This is the database name\n",
        "            self.collection_name = \"documents\"\n",
        "            self.setup_vector_store()\n",
        "            self.groq_client = groq.Client(api_key=\"INSERT API KEY HERE\")\n",
        "            self.document_loaded = False\n",
        "            self.chat_history = []\n",
        "            self.initialize_chat()\n",
        "\n",
        "            # Add storage for last retrieved chunks\n",
        "            self.last_retrieved_chunks = []\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Initialization error: {e}\")\n",
        "            raise\n",
        "\n",
        "    def initialize_chat(self):\n",
        "        \"\"\"Initialize chat history with system message\"\"\"\n",
        "        self.chat_history = [{\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"\"\"You are a helpful AI assistant specialized in answering questions based on provided document contexts.\n",
        "            Use the following contexts to answer the question accurately. If you cannot find the relevant information in the contexts,\n",
        "            say so honestly. Base your answer solely on the provided contexts while maintaining a natural conversational tone.\"\"\"\n",
        "        }]\n",
        "\n",
        "    def setup_vector_store(self):\n",
        "        \"\"\"Set up the vector store with proper schema and index\"\"\"\n",
        "        try:\n",
        "            # Drop existing collection if it exists\n",
        "            try:\n",
        "                if self.collection_name in self.client.list_collections():\n",
        "                    self.client.drop_collection(self.collection_name)\n",
        "            except Exception as e:\n",
        "                print(f\"Warning during collection check/drop: {e}\")\n",
        "\n",
        "            # Define collection schema\n",
        "            fields = [\n",
        "                FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=False),\n",
        "                FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535),\n",
        "                FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=self.embedding_dim)\n",
        "            ]\n",
        "            schema = CollectionSchema(fields=fields)\n",
        "\n",
        "            # Create collection\n",
        "            self.client.create_collection(\n",
        "                collection_name=self.collection_name,\n",
        "                schema=schema\n",
        "            )\n",
        "\n",
        "            # Create index for similarity search\n",
        "            index_params = {\n",
        "                \"field_name\": \"embedding\",\n",
        "                \"index_type\": \"HNSW\",\n",
        "                \"metric_type\": \"COSINE\",\n",
        "                \"params\": {\n",
        "                    \"M\": 8,\n",
        "                    \"efConstruction\": 64\n",
        "                }\n",
        "            }\n",
        "\n",
        "            self.client.create_index(\n",
        "                collection_name=self.collection_name,\n",
        "                index_params=[index_params]\n",
        "            )\n",
        "\n",
        "            # Load collection for search\n",
        "            self.client.load_collection(self.collection_name)\n",
        "\n",
        "            print(f\"Successfully set up vector store: {self.collection_name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Vector store setup error: {e}\")\n",
        "            raise\n",
        "\n",
        "    def retrieve_relevant_chunks(self, query: str, top_k: int = 3) -> List[str]:\n",
        "        \"\"\"Retrieve relevant chunks using similarity search\"\"\"\n",
        "        if not self.document_loaded:\n",
        "            raise ValueError(\"No document has been loaded yet\")\n",
        "\n",
        "        if not query.strip():\n",
        "            raise ValueError(\"Query cannot be empty\")\n",
        "\n",
        "        try:\n",
        "            # Generate query embedding\n",
        "            query_embedding = self.embedding_model.encode(query)\n",
        "\n",
        "            # Search in Milvus\n",
        "            search_params = {\n",
        "                \"metric_type\": \"COSINE\",\n",
        "                \"params\": {\"nprobe\": 10}\n",
        "            }\n",
        "\n",
        "            results = self.client.search(\n",
        "                collection_name=self.collection_name,\n",
        "                data=[query_embedding.tolist()],\n",
        "                limit=top_k,\n",
        "                output_fields=[\"text\"],\n",
        "                search_params=search_params\n",
        "            )\n",
        "\n",
        "            if not results or not results[0]:\n",
        "                print(\"No search results found\")\n",
        "                return []\n",
        "\n",
        "            relevant_chunks = []\n",
        "            for hit in results[0]:\n",
        "                if isinstance(hit, dict) and \"entity\" in hit:\n",
        "                    chunk = hit[\"entity\"].get(\"text\", \"\")\n",
        "                    if isinstance(chunk, str) and chunk.strip():\n",
        "                        relevant_chunks.append(chunk)\n",
        "\n",
        "            return relevant_chunks\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in retrieve_relevant_chunks: {e}\")\n",
        "            return []\n",
        "\n",
        "    def process_and_store_document(self, file_path: str) -> int:\n",
        "        \"\"\"Process document and store chunks in vector store\"\"\"\n",
        "        try:\n",
        "            # Reset state\n",
        "            self.document_loaded = False\n",
        "            self.clear_history()\n",
        "\n",
        "            # Process document\n",
        "            text = self.doc_processor.read_pdf(file_path)\n",
        "            if not text.strip():\n",
        "                raise ValueError(\"No text extracted from PDF\")\n",
        "\n",
        "            chunks = self.doc_processor.chunk_text(text)\n",
        "            if not chunks:\n",
        "                raise ValueError(\"No chunks created from document\")\n",
        "\n",
        "            # Generate embeddings\n",
        "            embeddings = self.embedding_model.encode(chunks)\n",
        "\n",
        "            # Clear existing data\n",
        "            try:\n",
        "                self.client.delete(\n",
        "                    collection_name=self.collection_name,\n",
        "                    filter=\"id >= 0\"\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"Warning during collection clearing: {e}\")\n",
        "\n",
        "            # Prepare entities\n",
        "            entities = []\n",
        "            for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
        "                if chunk.strip():  # Only add non-empty chunks\n",
        "                    entities.append({\n",
        "                        \"id\": i,\n",
        "                        \"text\": chunk,\n",
        "                        \"embedding\": embedding.tolist()\n",
        "                    })\n",
        "\n",
        "            # Insert data\n",
        "            if entities:\n",
        "                self.client.insert(\n",
        "                    collection_name=self.collection_name,\n",
        "                    data=entities\n",
        "                )\n",
        "                self.document_loaded = True\n",
        "                return len(entities)\n",
        "            else:\n",
        "                raise ValueError(\"No valid entities to insert\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Document processing error: {e}\")\n",
        "            self.document_loaded = False\n",
        "            raise\n",
        "\n",
        "    def generate_response(self, query: str, contexts: List[str]) -> str:\n",
        "        \"\"\"Generate response using Groq with retrieved contexts and chat history\"\"\"\n",
        "        try:\n",
        "            # Format context for better readability\n",
        "            formatted_context = \"\\n\\n\".join([f\"Context {i+1}:\\n{ctx}\" for i, ctx in enumerate(contexts)])\n",
        "\n",
        "            # Add user query to chat history\n",
        "            self.chat_history.append({\n",
        "                \"role\": \"user\",\n",
        "                \"content\": query\n",
        "            })\n",
        "\n",
        "            # Prepare messages for Groq with chat history context\n",
        "            messages = self.chat_history.copy()\n",
        "\n",
        "            # Add context to the latest user message\n",
        "            messages[-1][\"content\"] = f\"\"\"Using the following contexts and our conversation history, please answer this question: {query}\n",
        "\n",
        "Contexts:\n",
        "{formatted_context}\"\"\"\n",
        "\n",
        "            # Generate response using Groq\n",
        "            chat_completion = self.groq_client.chat.completions.create(\n",
        "                messages=messages,\n",
        "                model=\"llama3-70b-8192\",\n",
        "                temperature=0.7,\n",
        "                max_tokens=1000,\n",
        "                top_p=0.9\n",
        "            )\n",
        "\n",
        "            response = chat_completion.choices[0].message.content\n",
        "\n",
        "            # Add assistant's response to chat history\n",
        "            self.chat_history.append({\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": response\n",
        "            })\n",
        "\n",
        "            return response\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in generate_response: {e}\")\n",
        "            error_msg = f\"Error generating response: {str(e)}\"\n",
        "\n",
        "            # Add error response to chat history\n",
        "            self.chat_history.append({\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": error_msg\n",
        "            })\n",
        "\n",
        "            return error_msg\n",
        "\n",
        "    def query(self, query_text: str) -> str:\n",
        "        \"\"\"Process a query through the RAG pipeline\"\"\"\n",
        "        try:\n",
        "            if not self.document_loaded:\n",
        "                return \"Please upload and process a document first.\"\n",
        "\n",
        "            if not query_text.strip():\n",
        "                return \"Please enter a valid question.\"\n",
        "\n",
        "            # Retrieve relevant chunks using similarity search\n",
        "            relevant_chunks = self.retrieve_relevant_chunks(query_text, top_k=3)\n",
        "\n",
        "            # Store the retrieved chunks\n",
        "            self.last_retrieved_chunks = relevant_chunks\n",
        "\n",
        "            if not relevant_chunks:\n",
        "                return \"I couldn't find relevant information to answer your question.\"\n",
        "\n",
        "            # Generate response using the retrieved chunks\n",
        "            response = self.generate_response(query_text, relevant_chunks)\n",
        "\n",
        "            return response\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in query method: {e}\")\n",
        "            return f\"Error processing query: {str(e)}\"\n",
        "\n",
        "    def get_last_retrieved_chunks(self) -> str:\n",
        "        \"\"\"Get the chunks retrieved for the last query\"\"\"\n",
        "        if not self.last_retrieved_chunks:\n",
        "            return \"No chunks have been retrieved yet. Try asking a question first.\"\n",
        "\n",
        "        formatted_chunks = []\n",
        "        for i, chunk in enumerate(self.last_retrieved_chunks, 1):\n",
        "            formatted_chunks.append(f\"Chunk {i}:\\n{chunk}\\n\")\n",
        "\n",
        "        return \"\\n\".join(formatted_chunks)\n",
        "\n",
        "    def get_all_chunks(self) -> str:\n",
        "        \"\"\"Get all document chunks from the database\"\"\"\n",
        "        if not self.document_loaded:\n",
        "            return \"No document has been loaded yet. Please upload and process a document first.\"\n",
        "\n",
        "        try:\n",
        "            results = self.client.query(\n",
        "                collection_name=self.collection_name,\n",
        "                filter=\"id >= 0\",\n",
        "                output_fields=[\"text\", \"id\"],\n",
        "                limit=1000\n",
        "            )\n",
        "\n",
        "            if not results:\n",
        "                return \"No chunks found in the database.\"\n",
        "\n",
        "            chunks = []\n",
        "            for result in results:\n",
        "                if isinstance(result, dict) and \"text\" in result:\n",
        "                    text = result[\"text\"]\n",
        "                    if isinstance(text, list):\n",
        "                        text = text[0]\n",
        "                    if isinstance(text, str) and text.strip():\n",
        "                        chunk_id = result.get(\"id\", \"unknown\")\n",
        "                        chunks.append(f\"Chunk {chunk_id}: {text}\")\n",
        "\n",
        "            return \"\\n\\n\".join(chunks) if chunks else \"No readable chunks found.\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error retrieving chunks: {e}\")\n",
        "            return f\"Error retrieving chunks: {str(e)}\"\n",
        "\n",
        "    def clear_history(self):\n",
        "        \"\"\"Reset conversation history\"\"\"\n",
        "        self.initialize_chat()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4QnkN0G3D-9"
      },
      "source": [
        "## Gradio Interface\n",
        "\n",
        "Set up the Gradio interface for interactive use of the RAG system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ObyPpUAL3D-9"
      },
      "outputs": [],
      "source": [
        "def create_gradio_interface():\n",
        "    rag_system = RAGSystem()\n",
        "\n",
        "    def process_file(file):\n",
        "        if file is None:\n",
        "            return \"Please upload a document first.\"\n",
        "        try:\n",
        "            num_chunks = rag_system.process_and_store_document(file.name)\n",
        "            return f\"Processed {num_chunks} chunks. Documents are stored in Milvus Lite database.\"\n",
        "        except Exception as e:\n",
        "            return f\"Error: {str(e)}\"\n",
        "\n",
        "    def process_query(query, history):\n",
        "        if not query:\n",
        "            return \"\", history\n",
        "\n",
        "        try:\n",
        "            if not rag_system.document_loaded:\n",
        "                response = \"Please upload and process a document first.\"\n",
        "            else:\n",
        "                response = rag_system.query(query)\n",
        "\n",
        "            history = history + [[query, response]]\n",
        "            return \"\", history\n",
        "        except Exception as e:\n",
        "            return \"\", history + [[query, f\"Error processing query: {str(e)}\"]]\n",
        "\n",
        "    with gr.Blocks() as demo:\n",
        "        gr.Markdown(\"# RAG Question-Answering System with Groq and Milvus Lite\")\n",
        "\n",
        "        with gr.Row():\n",
        "            file_input = gr.File(label=\"Upload PDF Document\")\n",
        "            process_button = gr.Button(\"Process Document\")\n",
        "\n",
        "        output_text = gr.Textbox(label=\"Processing Status\")\n",
        "        process_button.click(process_file, inputs=[file_input], outputs=[output_text])\n",
        "\n",
        "        chatbot = gr.Chatbot()\n",
        "        msg = gr.Textbox(label=\"Enter your question\")\n",
        "        clear = gr.Button(\"Clear\")\n",
        "\n",
        "        msg.submit(process_query, [msg, chatbot], [msg, chatbot])\n",
        "        clear.click(lambda: (None, None), None, [msg, chatbot], queue=False)\n",
        "\n",
        "        with gr.Row():\n",
        "            col1, col2 = gr.Column(), gr.Column()\n",
        "            with col1:\n",
        "                display_all_chunks_button = gr.Button(\"Display All Document Chunks\")\n",
        "                all_chunks_output = gr.Textbox(label=\"All Document Chunks\", lines=10)\n",
        "            with col2:\n",
        "                display_retrieved_chunks_button = gr.Button(\"Display Last Retrieved Chunks\")\n",
        "                retrieved_chunks_output = gr.Textbox(label=\"Last Retrieved Chunks\", lines=10)\n",
        "\n",
        "        # Link buttons to their respective functions\n",
        "        display_all_chunks_button.click(\n",
        "            rag_system.get_all_chunks,\n",
        "            outputs=[all_chunks_output]\n",
        "        )\n",
        "\n",
        "        display_retrieved_chunks_button.click(\n",
        "            rag_system.get_last_retrieved_chunks,\n",
        "            outputs=[retrieved_chunks_output]\n",
        "        )\n",
        "\n",
        "    return demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGtWw4JL3D--"
      },
      "source": [
        "## Launch the Application\n",
        "\n",
        "Run this cell to start the Gradio interface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dYlRMi03D-_"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    demo = create_gradio_interface()\n",
        "    demo.launch(share=True,debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
